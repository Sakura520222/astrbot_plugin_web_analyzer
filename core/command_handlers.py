"""
å‘½ä»¤å¤„ç†å™¨æ¨¡å—

ä½¿ç”¨ CommandMixin æ¨¡å¼ï¼Œæä¾›æ‰€æœ‰çš„æ’ä»¶å‘½ä»¤å¤„ç†æ–¹æ³•ã€‚
è¿™äº›æ–¹æ³•å°†ä½œä¸ºæ··å…¥ç±»è¢«ä¸»æ’ä»¶ç±»ç»§æ‰¿ï¼Œä¿æŒå¯¹ self çš„å®Œæ•´è®¿é—®ã€‚
"""

import json
import os
import time

from astrbot.api import logger
from astrbot.api.event import AstrMessageEvent, filter
from astrbot.api.message_components import File, Node, Nodes, Plain

from core.plugin_helpers import PluginHelpers


class CommandMixin:
    """å‘½ä»¤å¤„ç†å™¨æ··å…¥ç±»

    åŒ…å«æ‰€æœ‰çš„æ’ä»¶å‘½ä»¤å¤„ç†æ–¹æ³•ï¼Œé€šè¿‡ç»§æ‰¿çš„æ–¹å¼æ··å…¥ä¸»æ’ä»¶ç±»ã€‚
    è¿™æ ·å¯ä»¥ä¿æŒå¯¹ä¸»ç±»å®ä¾‹å±æ€§çš„å®Œæ•´è®¿é—®ã€‚
    """

    @filter.command("web_help", alias={"ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"})
    async def show_help(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ’ä»¶çš„æ‰€æœ‰å¯ç”¨å‘½ä»¤å’Œå¸®åŠ©ä¿¡æ¯"""
        help_text = """ã€ç½‘é¡µåˆ†ææ’ä»¶å‘½ä»¤å¸®åŠ©ã€‘

ğŸ“‹ æ ¸å¿ƒåˆ†æå‘½ä»¤
ğŸ” /ç½‘é¡µåˆ†æ <URL1> <URL2>... - æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥
   åˆ«åï¼š/åˆ†æ, /æ€»ç»“, /web, /analyze
   ç¤ºä¾‹ï¼š/ç½‘é¡µåˆ†æ https://example.com

ğŸ“‹ é…ç½®ç®¡ç†å‘½ä»¤
ğŸ› ï¸ /web_config - æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®
   åˆ«åï¼š/ç½‘é¡µåˆ†æé…ç½®, /ç½‘é¡µåˆ†æè®¾ç½®
   ç¤ºä¾‹ï¼š/web_config

ğŸ“‹ ç¼“å­˜ç®¡ç†å‘½ä»¤
ğŸ—‘ï¸ /web_cache [clear] - ç®¡ç†åˆ†æç»“æœç¼“å­˜
   åˆ«åï¼š/ç½‘é¡µç¼“å­˜, /æ¸…ç†ç¼“å­˜
   é€‰é¡¹ï¼š
     - clear: æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
   ç¤ºä¾‹ï¼š/web_cache clear

ğŸ“‹ ç¾¤èŠç®¡ç†å‘½ä»¤
ğŸ‘¥ /group_blacklist [add/remove/clear] <ç¾¤å·> - ç®¡ç†ç¾¤èŠé»‘åå•
   åˆ«åï¼š/ç¾¤é»‘åå•, /é»‘åå•
   é€‰é¡¹ï¼š
     - (ç©º): æŸ¥çœ‹å½“å‰é»‘åå•
     - add <ç¾¤å·>: æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
     - remove <ç¾¤å·>: ä»é»‘åå•ç§»é™¤ç¾¤èŠ
     - clear: æ¸…ç©ºé»‘åå•
   ç¤ºä¾‹ï¼š/ç¾¤é»‘åå• add 123456789

ğŸ“‹ å¯¼å‡ºåŠŸèƒ½å‘½ä»¤
ğŸ“¤ /web_export - å¯¼å‡ºåˆ†æç»“æœ
   åˆ«åï¼š/å¯¼å‡ºåˆ†æç»“æœ, /ç½‘é¡µå¯¼å‡º
   ç¤ºä¾‹ï¼š/web_export

ğŸ“‹ æµ‹è¯•åŠŸèƒ½å‘½ä»¤
ğŸ“‹ /test_merge - æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½
   åˆ«åï¼š/æµ‹è¯•åˆå¹¶è½¬å‘, /æµ‹è¯•è½¬å‘
   ç¤ºä¾‹ï¼š/test_merge

ğŸ“‹ å¸®åŠ©å‘½ä»¤
â“ /web_help - æ˜¾ç¤ºæœ¬å¸®åŠ©ä¿¡æ¯
   åˆ«åï¼š/ç½‘é¡µåˆ†æå¸®åŠ©, /ç½‘é¡µåˆ†æå‘½ä»¤
   ç¤ºä¾‹ï¼š/web_help

ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
- æ‰€æœ‰å‘½ä»¤æ”¯æŒTabè¡¥å…¨ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
- å‘½ä»¤å‚æ•°æ”¯æŒæç¤ºåŠŸèƒ½
- å¯ä»¥è‡ªå®šä¹‰å‘½ä»¤åˆ«å

ğŸ”§ é…ç½®æç¤ºï¼š
- åœ¨AstrBotç®¡ç†é¢æ¿ä¸­å¯ä»¥é…ç½®æ’ä»¶çš„å„é¡¹åŠŸèƒ½
- æ”¯æŒè‡ªå®šä¹‰å‘½ä»¤åˆ«å
- å¯ä»¥è°ƒæ•´åˆ†æç»“æœæ¨¡æ¿å’Œæ˜¾ç¤ºæ–¹å¼
"""

        yield event.plain_result(help_text)
        logger.info("æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯")

    @filter.command("web_config", alias={"ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶çš„è¯¦ç»†é…ç½®ä¿¡æ¯"""
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.request_timeout_s} ç§’
- LLMæ™ºèƒ½åˆ†æ: {"âœ… å·²å¯ç”¨" if self.llm_enabled else "âŒ å·²ç¦ç”¨"}
- åˆ†ææ¨¡å¼: {self.analysis_mode}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {"âœ… å·²å¯ç”¨" if self.auto_analyze else "âŒ å·²ç¦ç”¨"}

**å¹¶å‘å¤„ç†è®¾ç½®**
- æœ€å¤§å¹¶å‘æ•°: {self.max_concurrency}
- LLMè‡ªä¸»å†³ç­–: {"âœ… å·²å¯ç”¨" if self.enable_llm_decision else "âŒ å·²ç¦ç”¨"}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {"âœ… å·²å¯ç”¨" if self.enable_emoji else "âŒ å·²ç¦ç”¨"}
- æ˜¾ç¤ºç»Ÿè®¡: {"âœ… å·²å¯ç”¨" if self.enable_statistics else "âŒ å·²ç¦ç”¨"}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å‘é€å†…å®¹ç±»å‹: {self.send_content_type}
- å¯ç”¨æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.enable_screenshot else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {"âœ… å·²å¯ç”¨" if self.enable_cache else "âŒ å·²ç¦ç”¨"}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time_min} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""

        yield event.plain_result(config_info)

    @filter.command("test_merge", alias={"æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = PluginHelpers.get_group_id(event)

        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []

            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")],
            )
            nodes.append(title_node)

            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node1)

            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node2)

            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")

    @filter.command("group_blacklist", alias={"ç¾¤é»‘åå•", "é»‘åå•"})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return

            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"

            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"

            yield event.plain_result(blacklist_info)
            return

        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""

        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")

        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")

        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return

            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result(
                "æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear"
            )

    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶"""
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = "\n".join(self.group_blacklist)
            # è·å–å½“å‰group_settingsé…ç½®
            group_settings = self.config.get("group_settings", {})
            # æ›´æ–°group_blacklist
            group_settings["group_blacklist"] = group_text
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config["group_settings"] = group_settings
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_cache", alias={"ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time_min} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += (
                f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            )

            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"

            yield event.plain_result(cache_info)
            return

        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""

        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(
                f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª"
            )

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_mode", alias={"åˆ†ææ¨¡å¼", "ç½‘é¡µåˆ†ææ¨¡å¼"})
    async def manage_analysis_mode(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†ææ¨¡å¼"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰æ¨¡å¼
        if len(message_parts) <= 1:
            mode_names = {
                "auto": "è‡ªåŠ¨åˆ†æ",
                "manual": "æ‰‹åŠ¨åˆ†æ",
                "hybrid": "æ··åˆæ¨¡å¼",
            }
            mode_info = "**å½“å‰åˆ†ææ¨¡å¼**\n\n"
            mode_info += f"- æ¨¡å¼: {mode_names.get(self.analysis_mode, self.analysis_mode)} ({self.analysis_mode})\n"
            mode_info += (
                f"- è‡ªåŠ¨åˆ†æ: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}\n\n"
            )
            mode_info += "ä½¿ç”¨ `/web_mode <æ¨¡å¼>` åˆ‡æ¢æ¨¡å¼\n"
            mode_info += "æ”¯æŒçš„æ¨¡å¼: auto, manual, hybrid, LLMTOOL"

            yield event.plain_result(mode_info)
            return

        # è§£ææ¨¡å¼å‚æ•°
        mode = message_parts[1].lower() if len(message_parts) > 1 else ""
        valid_modes = ["auto", "manual", "hybrid", "LLMTOOL"]

        # éªŒè¯æ¨¡å¼æ˜¯å¦æœ‰æ•ˆ
        if mode not in valid_modes:
            yield event.plain_result(f"æ— æ•ˆçš„æ¨¡å¼ï¼Œè¯·ä½¿ç”¨: {', '.join(valid_modes)}")
            return

        # æ›´æ–°åˆ†ææ¨¡å¼
        self.analysis_mode = mode
        self.auto_analyze = mode == "auto"

        yield event.plain_result(f"âœ… å·²åˆ‡æ¢åˆ° {mode} æ¨¡å¼")

    @filter.command("web_export", alias={"å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result(
                "è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json"
            )
            return

        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"

        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(
                f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}"
            )
            return

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []

        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache_manager.memory_cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return

            for url, cache_data in self.cache_manager.memory_cache.items():
                export_results.append({"url": url, "result": cache_data["result"]})
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all

            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self.message_handler.check_cache(url)
            if cached_result:
                export_results.append({"url": url, "result": cached_result})
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")

                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with self.analyzer as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return

                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(
                            event, content_data["content"]
                        )
                        translated_content_data = content_data.copy()
                        translated_content_data["content"] = translated_content
                        analysis_result = await self.llm_analyzer.analyze_with_llm(
                            event, translated_content_data
                        )
                    else:
                        analysis_result = await self.llm_analyzer.analyze_with_llm(
                            event, content_data
                        )

                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.enable_specific_extraction:
                        specific_content = analyzer.extract_specific_content(
                            html, url, self.extract_types
                        )
                        if specific_content:
                            # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                            analysis_result = self._add_specific_content_to_result(
                                analysis_result, specific_content
                            )

                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append(
                        {
                            "url": url,
                            "result": {
                                "url": url,
                                "result": analysis_result,
                                "screenshot": None,
                            },
                        }
                    )

        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url_obj = export_results[0]["url"]
                from urllib.parse import urlparse

                parsed = urlparse(url_obj)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"

            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"

            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")

            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)

            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(timestamp)
                    ),
                    "total_results": len(export_results),
                    "results": [],
                }

                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]

                    json_data["results"].append(
                        {
                            "url": url,
                            "analysis_result": result_data["result"],
                            "has_screenshot": result_data["screenshot"] is not None,
                        }
                    )

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                File(file=file_path, name=os.path.basename(file_path)),
            ]

            yield event.chain_result(message_chain)

            logger.info(
                f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·"
            )

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")

    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content: è¦ç¿»è¯‘çš„å†…å®¹

        Returns:
            ç¿»è¯‘åçš„å†…å®¹
        """
        if not self.enable_translation:
            return content

        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, "llm_generate"):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(
                    umo=umo
                )

            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content, target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = (
                    f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œ"
                    f"ä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"
                )

            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id, prompt=prompt
            )

            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content

    def _add_specific_content_to_result(
        self, analysis_result: str, specific_content: dict
    ) -> str:
        """å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­

        Args:
            analysis_result: å½“å‰çš„åˆ†æç»“æœ
            specific_content: ç‰¹å®šå†…å®¹å­—å…¸

        Returns:
            æ›´æ–°åçš„åˆ†æç»“æœ
        """
        try:
            # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
            specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

            # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "images" in specific_content and specific_content["images"]:
                specific_content_str += (
                    f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                )
                for img in specific_content["images"]:
                    img_url = img.get("url", "")
                    alt_text = img.get("alt", "")
                    if alt_text:
                        specific_content_str += f"- {img_url} (alt: {alt_text})\n"
                    else:
                        specific_content_str += f"- {img_url}\n"

            # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "links" in specific_content and specific_content["links"]:
                specific_content_str += (
                    f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                )
                for link in specific_content["links"][:5]:
                    specific_content_str += f"- [{link['text']}]({link['url']})\n"

            # æ·»åŠ è§†é¢‘é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "videos" in specific_content and specific_content["videos"]:
                specific_content_str += (
                    f"\nğŸ¬ è§†é¢‘é“¾æ¥ ({len(specific_content['videos'])}):\n"
                )
                for video in specific_content["videos"]:
                    video_url = video.get("url", "")
                    video_type = video.get("type", "video")
                    specific_content_str += f"- {video_url} (type: {video_type})\n"

            # æ·»åŠ éŸ³é¢‘é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "audios" in specific_content and specific_content["audios"]:
                specific_content_str += (
                    f"\nğŸµ éŸ³é¢‘é“¾æ¥ ({len(specific_content['audios'])}):\n"
                )
                for audio in specific_content["audios"]:
                    specific_content_str += f"- {audio}\n"

            # æ·»åŠ å¼•ç”¨å—ï¼ˆå¦‚æœæœ‰ï¼‰
            if "quotes" in specific_content and specific_content["quotes"]:
                specific_content_str += (
                    f"\nğŸ’¬ å¼•ç”¨å— ({len(specific_content['quotes'])}):\n"
                )
                for quote in specific_content["quotes"][:3]:
                    quote_text = quote.get("text", "")
                    author = quote.get("author", "")
                    if author:
                        specific_content_str += f"> {quote_text} â€” {author}\n\n"
                    else:
                        specific_content_str += f"> {quote_text}\n\n"

            # æ·»åŠ æ ‡é¢˜åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if "headings" in specific_content and specific_content["headings"]:
                specific_content_str += (
                    f"\nğŸ“‘ æ ‡é¢˜åˆ—è¡¨ ({len(specific_content['headings'])}):\n"
                )
                for heading in specific_content["headings"]:
                    level = heading.get("level", 1)
                    text = heading.get("text", "")
                    heading_id = heading.get("id", "")
                    indent = "  " * (level - 1)
                    if heading_id:
                        specific_content_str += (
                            f"{indent}#{level} {text} (id: {heading_id})\n"
                        )
                    else:
                        specific_content_str += f"{indent}#{level} {text}\n"

            # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼‰
            if "code_blocks" in specific_content and specific_content["code_blocks"]:
                specific_content_str += (
                    f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                )
                for code_block in specific_content["code_blocks"][:2]:
                    code = code_block.get("code", "")
                    language = code_block.get("language", "")
                    specific_content_str += f"``` {language}\n{code}\n```\n"

            # æ·»åŠ è¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰
            if "tables" in specific_content and specific_content["tables"]:
                specific_content_str += f"\nğŸ“Š è¡¨æ ¼ ({len(specific_content['tables'])}):\n"
                for table in specific_content["tables"][:2]:
                    headers = table.get("headers", [])
                    rows = table.get("rows", [])
                    specific_content_str += f"\nè¡¨æ ¼:\n"
                    if headers:
                        specific_content_str += (
                            f"| {' | '.join(headers)} |\n"
                            f"| {' | '.join(['---' for _ in headers])} |\n"
                        )
                    for row in rows:
                        specific_content_str += f"| {' | '.join(row)} |\n"

            # æ·»åŠ åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if "lists" in specific_content and specific_content["lists"]:
                specific_content_str += f"\nğŸ“‹ åˆ—è¡¨ ({len(specific_content['lists'])}):\n"
                for list_item in specific_content["lists"][:2]:
                    list_type = list_item.get("type", "ul")
                    items = list_item.get("items", [])
                    specific_content_str += f"\nåˆ—è¡¨ ({list_type}):\n"
                    for item in items:
                        if list_type == "ol":
                            specific_content_str += f"1. {item}\n"
                        else:
                            specific_content_str += f"- {item}\n"

            # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "meta" in specific_content and specific_content["meta"]:
                meta_info = specific_content["meta"]
                specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                for key, value in meta_info.items():
                    if value:
                        specific_content_str += f"- {key}: {value}\n"

            # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
            return analysis_result + specific_content_str
        except Exception as e:
            logger.warning(f"æ·»åŠ ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return analysis_result