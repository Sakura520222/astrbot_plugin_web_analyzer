# -*- coding: utf-8 -*-
"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶ - é‡æ„ç‰ˆæœ¬

è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥ï¼Œæ™ºèƒ½æŠ“å–è§£æå†…å®¹ï¼Œé›†æˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æå’Œæ€»ç»“ï¼Œ
æ”¯æŒç½‘é¡µæˆªå›¾ã€ç¼“å­˜æœºåˆ¶å’Œå¤šç§ç®¡ç†å‘½ä»¤ã€‚

æœ¬ç‰ˆæœ¬ä½¿ç”¨æ ¸å¿ƒæ¨¡å—é‡æ„ï¼Œéµå¾ª PEP 8 è§„èŒƒã€‚
"""

import sys
from pathlib import Path

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ core æ¨¡å—ä¹‹å‰ï¼‰
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from typing import Any

from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, filter
from astrbot.api.star import Context, Star, register

from core.analyzer import WebAnalyzer
from core.cache import CacheManager
from core.utils import WebAnalyzerUtils

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.constants import ErrorType
from core.config_loader import ConfigLoader
from core.error_handler import ErrorHandler
from core.result_formatter import ResultFormatter
from core.llm_analyzer import LLMAnalyzer
from core.message_handler import MessageHandler


@register(
    "astrbot_plugin_web_analyzer",
    "Sakura520222",
    "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥ï¼Œæ™ºèƒ½æŠ“å–è§£æå†…å®¹ï¼Œé›†æˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æå’Œæ€»ç»“ï¼Œæ”¯æŒç½‘é¡µæˆªå›¾ã€ç¼“å­˜æœºåˆ¶å’Œå¤šç§ç®¡ç†å‘½ä»¤",
    "1.4.5",
    "https://github.com/Sakura520222/astrbot_plugin_web_analyzer",
)
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œè°ƒåº¦æ‰€æœ‰åŠŸèƒ½æ¨¡å—"""

    def __init__(self, context: Context, config: AstrBotConfig):
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•ï¼Œè´Ÿè´£åŠ è½½ã€éªŒè¯å’Œåˆå§‹åŒ–æ‰€æœ‰é…ç½®é¡¹"""
        super().__init__(context)
        self.config = config

        # ä½¿ç”¨é…ç½®åŠ è½½å™¨åŠ è½½æ‰€æœ‰é…ç½®
        config_dict = ConfigLoader.load_all_config(config, context)

        # å°†é…ç½®é¡¹è®¾ç½®ä¸ºå®ä¾‹å±æ€§
        for key, value in config_dict.items():
            setattr(self, key, value)

        # URLå¤„ç†æ ‡å¿—é›†åˆï¼šç”¨äºé¿å…é‡å¤å¤„ç†åŒä¸€URL
        self.processing_urls = set()

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self._init_components(context, config_dict)

        # æ’¤å›ä»»åŠ¡åˆ—è¡¨ï¼šç”¨äºç®¡ç†æ‰€æœ‰æ’¤å›ä»»åŠ¡
        self.recall_tasks = []

        # è®°å½•é…ç½®åˆå§‹åŒ–å®Œæˆ
        logger.info("æ’ä»¶é…ç½®åˆå§‹åŒ–å®Œæˆ")

    def _init_components(self, context: Context, config_dict: dict):
        """åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶

        Args:
            context: AstrBot ä¸Šä¸‹æ–‡å¯¹è±¡
            config_dict: é…ç½®å­—å…¸
        """
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size,
            expire_time=self.cache_expire_time,
            preload_enabled=self.cache_preload_enabled,
            preload_count=self.cache_preload_count,
        )

        # åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length,
            timeout=self.timeout,
            user_agent=self.user_agent,
            proxy=self.proxy,
            retry_count=self.retry_count,
            retry_delay=self.retry_delay,
            enable_memory_monitor=self.enable_memory_monitor,
            memory_threshold=self.memory_threshold,
            enable_unified_domain=self.enable_unified_domain,
        )

        # åˆå§‹åŒ–ç»“æœæ ¼å¼åŒ–å™¨
        self.result_formatter = ResultFormatter(
            enable_emoji=self.enable_emoji,
            enable_statistics=self.enable_statistics,
        )

        # åˆå§‹åŒ– LLM åˆ†æå™¨
        self.llm_analyzer = LLMAnalyzer(
            context=context,
            llm_provider=self.llm_provider,
            custom_prompt=self.custom_prompt,
            max_summary_length=self.max_summary_length,
            enable_emoji=self.enable_emoji,
        )

        # åˆå§‹åŒ–æ¶ˆæ¯å¤„ç†å™¨
        self.message_handler = MessageHandler(
            analyzer=self.analyzer,
            cache_manager=self.cache_manager,
            enable_cache=self.enable_cache,
            enable_screenshot=self.enable_screenshot,
            send_content_type=self.send_content_type,
            screenshot_format=self.screenshot_format,
            merge_forward_group=self.merge_forward_group,
            merge_forward_private=self.merge_forward_private,
            merge_forward_include_screenshot=self.merge_forward_include_screenshot,
        )

    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist

    def _get_group_id(self, event: AstrMessageEvent) -> str | None:
        """ä»äº‹ä»¶å¯¹è±¡ä¸­è·å–ç¾¤èŠIDï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬çš„äº‹ä»¶å¯¹è±¡"""
        group_id = None
        if hasattr(event, "unified_msg_origin"):
            umo = event.unified_msg_origin
            if isinstance(umo, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ— æ³•è·å– group_id
                group_id = None
            elif hasattr(umo, "group_id"):
                group_id = umo.group_id
        elif hasattr(event, "group_id"):
            group_id = event.group_id
        return group_id

    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šURLçš„åŸŸåæ˜¯å¦å…è®¸è®¿é—®"""
        return WebAnalyzerUtils.is_domain_allowed(
            url, self.allowed_domains, self.blocked_domains
        )

    def _handle_error(
        self,
        error_type: str,
        original_error: Exception,
        url: str | None = None,
        context: dict | None = None,
    ) -> str:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†æ–¹æ³•"""
        return ErrorHandler.handle_error(error_type, original_error, url, context)

    def _get_error_type(self, exception: Exception) -> str:
        """æ ¹æ®å¼‚å¸¸ç±»å‹è·å–å¯¹åº”çš„é”™è¯¯ç±»å‹"""
        return ErrorHandler.get_error_type(exception)

    def _apply_result_settings(
        self, result: str, url: str, content_data: dict = None
    ) -> str:
        """åº”ç”¨æ‰€æœ‰ç»“æœè®¾ç½®"""
        return self.result_formatter.apply_result_settings(
            result=result,
            url=url,
            content_data=content_data,
            enable_custom_template=self.enable_custom_template,
            result_template=self.result_template,
            enable_collapsible=self.enable_collapsible,
            collapse_threshold=self.collapse_threshold,
            template_content=self.template_content,
        )

    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æ"""
        return self.result_formatter.build_enhanced_analysis(content_data)

    @filter.command("ç½‘é¡µåˆ†æ", alias={"åˆ†æ", "æ€»ç»“", "web", "analyze"})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è§¦å‘ç½‘é¡µåˆ†æå‘½ä»¤"""
        message_text = event.message_str

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(
            message_text, self.enable_no_protocol_url, self.default_protocol
        )
        if not urls:
            yield event.plain_result(
                "è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com"
            )
            return

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = [
            self.analyzer.normalize_url(url)
            for url in urls
            if self.analyzer.is_valid_url(url)
        ]
        valid_urls = list(set(valid_urls))
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return

        # æ£€æŸ¥ç¾¤ç»„é»‘åå•
        group_id = self._get_group_id(event)
        if self._is_group_blacklisted(str(group_id) if group_id else ""):
            logger.info(f"ç¾¤ç»„ {group_id} åœ¨é»‘åå•ä¸­ï¼Œè·³è¿‡å¤„ç†")
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯
        if len(allowed_urls) == 1:
            message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}"
        else:
            message = f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥..."

        processing_message_id, bot = await self._send_processing_message(event, message)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(
            event, allowed_urls, processing_message_id, bot
        ):
            yield result

    @filter.llm_tool(name="analyze_webpage")
    async def analyze_webpage_tool(self, event: AstrMessageEvent, url: str) -> Any:
        """æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…·

        Args:
            url(string): è¦åˆ†æçš„ç½‘é¡µURLåœ°å€ï¼Œæ”¯æŒhttpå’Œhttpsåè®®
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†LLMTOOLæ¨¡å¼ï¼Œæœªå¯ç”¨åˆ™ä¸æ‰§è¡Œ
        if self.analysis_mode != "LLMTOOL":
            logger.info(f"å½“å‰æœªå¯ç”¨LLMTOOLæ¨¡å¼ï¼Œæ‹’ç»analyze_webpage_toolè°ƒç”¨: {url}")
            yield event.plain_result("å½“å‰æœªå¯ç”¨ç½‘é¡µåˆ†æå·¥å…·æ¨¡å¼")
            return

        logger.info(f"æ”¶åˆ°analyze_webpage_toolè°ƒç”¨ï¼ŒåŸå§‹URL: {url}")

        # é¢„å¤„ç†URLï¼šå»é™¤å¯èƒ½çš„åå¼•å·ã€ç©ºæ ¼ç­‰
        processed_url = url.strip().strip("`")
        logger.info(f"é¢„å¤„ç†åçš„URL: {processed_url}")

        # è¡¥å…¨URLåè®®å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not processed_url.startswith(("http://", "https://")):
            processed_url = f"{self.default_protocol}://{processed_url}"
            logger.info(f"è¡¥å…¨åè®®å¤´åçš„URL: {processed_url}")

        # è§„èŒƒåŒ–URL
        normalized_url = self.analyzer.normalize_url(processed_url)
        logger.info(f"è§„èŒƒåŒ–åçš„URL: {normalized_url}")

        if not self.analyzer.is_valid_url(normalized_url):
            error_msg = f"æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # æ£€æŸ¥åŸŸåæ˜¯å¦å…è®¸è®¿é—®
        if not self._is_domain_allowed(normalized_url):
            error_msg = f"è¯¥åŸŸåä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {normalized_url}"
        processing_message_id, bot = await self._send_processing_message(event, message)

        # å¤„ç†å•ä¸ªURL
        async for result in self._batch_process_urls(
            event, [normalized_url], processing_message_id, bot
        ):
            yield result

    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ"""
        # æ£€æŸ¥åˆ†ææ¨¡å¼ï¼Œmanualæ¨¡å¼ä¸‹ä¸è¿›è¡Œè‡ªåŠ¨åˆ†æ
        if self.analysis_mode == "manual":
            return

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æåŠŸèƒ½ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
        if not self.auto_analyze:
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡ä»¤è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
        message_text = event.message_str.strip()

        # è·³è¿‡ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        if message_text.startswith("/"):
            logger.info("æ£€æµ‹åˆ°æŒ‡ä»¤è°ƒç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ£€æŸ¥äº‹ä»¶æ˜¯å¦æœ‰commandå±æ€§ï¼ˆæŒ‡ä»¤è°ƒç”¨æ—¶ä¼šæœ‰ï¼‰
        if hasattr(event, "command"):
            logger.info("æ£€æµ‹åˆ°commandå±æ€§ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        group_id = self._get_group_id(event)
        if self._is_group_blacklisted(str(group_id) if group_id else ""):
            return

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(
            message_text, self.enable_no_protocol_url, self.default_protocol
        )
        if not urls:
            return

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = list(
            set(
                [
                    self.analyzer.normalize_url(url)
                    for url in urls
                    if self.analyzer.is_valid_url(url)
                ]
            )
        )
        if not valid_urls:
            return

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return

        # æ ¹æ®analysis_modeé…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†ææ–¹å¼
        if self.analysis_mode == "LLMTOOL":
            # å¯ç”¨äº†LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä¸ä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†æ
            logger.info(
                f"å¯ç”¨äº†LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä¸è‡ªåŠ¨åˆ†æé“¾æ¥ï¼Œè®©LLMè‡ªå·±å†³å®š: {allowed_urls}"
            )
            return
        else:
            # æœªå¯ç”¨LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†ææ–¹å¼
            # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
            if len(allowed_urls) == 1:
                message = f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}"
            else:
                message = f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ..."

            # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yieldï¼Œè·å–message_idå’Œbotå®ä¾‹
            processing_message_id, bot = await self._send_processing_message(
                event, message
            )

            # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
            async for result in self._batch_process_urls(
                event, allowed_urls, processing_message_id, bot
            ):
                yield result

    async def _batch_process_urls(
        self,
        event: AstrMessageEvent,
        urls: list,
        processing_message_id: int,
        bot: Any,
    ):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
        results = []

        for url in urls:
            try:
                # æ ‡è®°URLæ­£åœ¨å¤„ç†ä¸­
                self.processing_urls.add(url)

                # ä½¿ç”¨æ¶ˆæ¯å¤„ç†å™¨å¤„ç†å•ä¸ªURL
                result = await self.message_handler.process_single_url(
                    event=event,
                    url=url,
                    analyzer=self.analyzer,
                    llm_analyzer=self.llm_analyzer if self.llm_enabled else None,
                    enable_translation=self.enable_translation,
                    enable_specific_extraction=self.enable_specific_extraction,
                    extract_types=self.extract_types,
                    result_formatter=self.result_formatter,
                )

                results.append(result)

                # ä»å¤„ç†ä¸­é›†åˆç§»é™¤URL
                self.processing_urls.discard(url)

            except Exception as e:
                error_type = self._get_error_type(e)
                error_msg = self._handle_error(error_type, e, url)
                results.append({"url": url, "result": error_msg, "screenshot": None})
                self.processing_urls.discard(url)

        # å‘é€æ‰€æœ‰åˆ†æç»“æœ
        if results:
            async for result in self.message_handler.send_analysis_result(event, results):
                yield result

        # æ’¤å›å¤„ç†æ¶ˆæ¯
        if self.enable_recall and processing_message_id and bot:
            await self._recall_processing_message(
                event, processing_message_id, bot, self.recall_time
            )

    async def _send_processing_message(self, event: AstrMessageEvent, message: str):
        """å‘é€å¤„ç†æç¤ºæ¶ˆæ¯å¹¶è®¾ç½®è‡ªåŠ¨æ’¤å›"""
        import asyncio

        # è·å–botå®ä¾‹ï¼ˆå…¼å®¹ä¸åŒç±»å‹çš„äº‹ä»¶ï¼‰
        bot = event.bot if hasattr(event, "bot") else None
        message_id = None

        # ç›´æ¥è°ƒç”¨botçš„å‘é€æ¶ˆæ¯æ–¹æ³•ï¼Œè·å–æ¶ˆæ¯ID
        try:
            # æ ¹æ®äº‹ä»¶ç±»å‹é€‰æ‹©å‘é€æ–¹æ³•
            send_result = None
            group_id = None
            user_id = None

            # æ–¹æ³•1ï¼šä½¿ç”¨AiocqhttpMessageEventçš„æ–¹æ³•è·å–
            if hasattr(event, "get_group_id"):
                group_id = event.get_group_id()
            if hasattr(event, "get_sender_id"):
                user_id = event.get_sender_id()

            # æ–¹æ³•2ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºç§èŠ
            is_private = False
            if hasattr(event, "is_private_chat"):
                is_private = event.is_private_chat()

            # å‘é€æ¶ˆæ¯
            if bot and group_id:
                # ç¾¤èŠæ¶ˆæ¯
                send_result = await bot.send_group_msg(
                    group_id=group_id, message=message
                )
                logger.debug(f"å‘é€ç¾¤èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç¾¤ {group_id}")
            elif bot and (user_id or is_private):
                # ç§èŠæ¶ˆæ¯
                if not user_id and hasattr(event, "get_sender_id"):
                    user_id = event.get_sender_id()

                if user_id:
                    send_result = await bot.send_private_msg(
                        user_id=user_id, message=message
                    )
                    logger.debug(f"å‘é€ç§èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç”¨æˆ· {user_id}")
                else:
                    # æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€
                    logger.warning(f"æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€æ¶ˆæ¯: {message}")
                    response = event.plain_result(message)
                    if hasattr(event, "send"):
                        await event.send(response)
                    return None, bot
            else:
                # æ— æ³•ç¡®å®šæ¶ˆæ¯ç±»å‹æˆ–æ²¡æœ‰botå®ä¾‹ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
                logger.debug(
                    f"ä½¿ç”¨åŸå§‹æ–¹å¼å‘é€å¤„ç†æ¶ˆæ¯ï¼Œeventç±»å‹: {type(event)}, has_bot={hasattr(event, 'bot')}, get_group_id={hasattr(event, 'get_group_id')}, get_sender_id={hasattr(event, 'get_sender_id')}, is_private_chat={hasattr(event, 'is_private_chat')}"
                )
                # å°è¯•ä½¿ç”¨event.plain_resultå‘é€ï¼Œè™½ç„¶æ— æ³•è·å–message_id
                response = event.plain_result(message)
                # ä½¿ç”¨eventçš„sendæ–¹æ³•å‘é€
                if hasattr(event, "send"):
                    await event.send(response)
                return None, bot

            # æ£€æŸ¥send_resultæ˜¯å¦åŒ…å«message_id
            if isinstance(send_result, dict):
                message_id = send_result.get("message_id")
            elif hasattr(send_result, "message_id"):
                message_id = send_result.message_id

            logger.debug(f"å‘é€å¤„ç†æ¶ˆæ¯æˆåŠŸï¼Œmessage_id: {message_id}")

            # å¦‚æœè·å–åˆ°message_idä¸”å¯ç”¨äº†è‡ªåŠ¨æ’¤å›ä¸”æœ‰botå®ä¾‹
            if message_id and self.enable_recall and bot:
                # å®šæ—¶æ’¤å›æ¨¡å¼
                if self.recall_type == "time_based":
                    logger.info(
                        f"åˆ›å»ºå®šæ—¶æ’¤å›ä»»åŠ¡ï¼Œmessage_id: {message_id}ï¼Œå»¶è¿Ÿ: {self.recall_time}ç§’"
                    )

                    async def _recall_task():
                        try:
                            await asyncio.sleep(self.recall_time)
                            await bot.delete_msg(message_id=message_id)
                            logger.info(f"å·²å®šæ—¶æ’¤å›æ¶ˆæ¯: {message_id}")
                        except Exception as e:
                            logger.error(f"å®šæ—¶æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")

                    task = asyncio.create_task(_recall_task())

                    # å°†ä»»åŠ¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­ç®¡ç†
                    self.recall_tasks.append(task)

                    # æ·»åŠ å®Œæˆå›è°ƒï¼Œä»åˆ—è¡¨ä¸­ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                    def _remove_task(t):
                        try:
                            self.recall_tasks.remove(t)
                        except ValueError:
                            pass

                    task.add_done_callback(_remove_task)
                # æ™ºèƒ½æ’¤å›æ¨¡å¼ - åªå‘é€æ¶ˆæ¯ï¼Œä¸åˆ›å»ºå®šæ—¶ä»»åŠ¡ï¼Œç­‰å¾…åˆ†æå®Œæˆåç«‹å³æ’¤å›
                elif self.recall_type == "smart" and self.smart_recall_enabled:
                    logger.info(
                        f"å·²å‘é€æ™ºèƒ½æ’¤å›æ¶ˆæ¯ï¼Œmessage_id: {message_id}ï¼Œç­‰å¾…åˆ†æå®Œæˆåç«‹å³æ’¤å›"
                    )

        except Exception as e:
            logger.error(f"å‘é€å¤„ç†æ¶ˆæ¯æˆ–è®¾ç½®æ’¤å›å¤±è´¥: {e}")

        return message_id, bot

    async def _recall_processing_message(
        self, event: AstrMessageEvent, message_id: str, bot: Any, delay: int
    ):
        """æ’¤å›å¤„ç†æç¤ºæ¶ˆæ¯
        
        æ ¹æ®é…ç½®çš„æ’¤å›ç±»å‹æ‰§è¡Œä¸åŒçš„æ’¤å›ç­–ç•¥ï¼š
        - smart: æ™ºèƒ½æ’¤å›ï¼Œåˆ†æå®Œæˆåç«‹å³æ’¤å›ï¼ˆæ— å»¶è¿Ÿï¼‰
        - time_based: å®šæ—¶æ’¤å›ï¼Œç­‰å¾…æŒ‡å®šæ—¶é—´åæ’¤å›
        """
        if not message_id or not bot:
            return

        try:
            # æ£€æŸ¥æ’¤å›ç±»å‹
            if (
                self.recall_type == "smart"
                and self.smart_recall_enabled
            ):
                # æ™ºèƒ½æ’¤å›ï¼šç«‹å³æ’¤å›ï¼Œä¸éœ€è¦å»¶è¿Ÿ
                logger.info(f"æ™ºèƒ½æ’¤å›ï¼šåˆ†æå®Œæˆï¼Œç«‹å³æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯ï¼Œmessage_id: {message_id}")
                await bot.delete_msg(message_id=message_id)
                logger.info(f"æ™ºèƒ½æ’¤å›æˆåŠŸï¼Œå·²æ’¤å›æ¶ˆæ¯: {message_id}")
            else:
                # å®šæ—¶æ’¤å›ï¼šç­‰å¾…æŒ‡å®šæ—¶é—´åæ’¤å›
                if delay > 0:
                    import asyncio
                    logger.info(f"å®šæ—¶æ’¤å›ï¼šç­‰å¾… {delay} ç§’åæ’¤å›æ¶ˆæ¯ï¼Œmessage_id: {message_id}")
                    await asyncio.sleep(delay)
                
                await bot.delete_msg(message_id=message_id)
                logger.info(f"å®šæ—¶æ’¤å›æˆåŠŸï¼Œå·²æ’¤å›æ¶ˆæ¯: {message_id}")
        except Exception as e:
            logger.error(f"æ’¤å›å¤„ç†æç¤ºæ¶ˆæ¯å¤±è´¥: {e}")

    @filter.command("web_help", alias={"ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"})
    async def show_help(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ’ä»¶çš„æ‰€æœ‰å¯ç”¨å‘½ä»¤å’Œå¸®åŠ©ä¿¡æ¯"""
        help_text = """ã€ç½‘é¡µåˆ†ææ’ä»¶å‘½ä»¤å¸®åŠ©ã€‘

ğŸ“‹ æ ¸å¿ƒåˆ†æå‘½ä»¤
ğŸ” /ç½‘é¡µåˆ†æ <URL1> <URL2>... - æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥
   åˆ«åï¼š/åˆ†æ, /æ€»ç»“, /web, /analyze
   ç¤ºä¾‹ï¼š/ç½‘é¡µåˆ†æ https://example.com

ğŸ“‹ é…ç½®ç®¡ç†å‘½ä»¤
ğŸ› ï¸ /web_config - æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®
   åˆ«åï¼š/ç½‘é¡µåˆ†æé…ç½®, /ç½‘é¡µåˆ†æè®¾ç½®
   ç¤ºä¾‹ï¼š/web_config

ğŸ“‹ ç¼“å­˜ç®¡ç†å‘½ä»¤
ğŸ—‘ï¸ /web_cache [clear] - ç®¡ç†åˆ†æç»“æœç¼“å­˜
   åˆ«åï¼š/ç½‘é¡µç¼“å­˜, /æ¸…ç†ç¼“å­˜
   é€‰é¡¹ï¼š
     - clear: æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
   ç¤ºä¾‹ï¼š/web_cache clear

ğŸ“‹ ç¾¤èŠç®¡ç†å‘½ä»¤
ğŸ‘¥ /group_blacklist [add/remove/clear] <ç¾¤å·> - ç®¡ç†ç¾¤èŠé»‘åå•
   åˆ«åï¼š/ç¾¤é»‘åå•, /é»‘åå•
   é€‰é¡¹ï¼š
     - (ç©º): æŸ¥çœ‹å½“å‰é»‘åå•
     - add <ç¾¤å·>: æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
     - remove <ç¾¤å·>: ä»é»‘åå•ç§»é™¤ç¾¤èŠ
     - clear: æ¸…ç©ºé»‘åå•
   ç¤ºä¾‹ï¼š/ç¾¤é»‘åå• add 123456789

ğŸ“‹ å¯¼å‡ºåŠŸèƒ½å‘½ä»¤
ğŸ“¤ /web_export - å¯¼å‡ºåˆ†æç»“æœ
   åˆ«åï¼š/å¯¼å‡ºåˆ†æç»“æœ, /ç½‘é¡µå¯¼å‡º
   ç¤ºä¾‹ï¼š/web_export

ğŸ“‹ æµ‹è¯•åŠŸèƒ½å‘½ä»¤
ğŸ“‹ /test_merge - æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½
   åˆ«åï¼š/æµ‹è¯•åˆå¹¶è½¬å‘, /æµ‹è¯•è½¬å‘
   ç¤ºä¾‹ï¼š/test_merge

ğŸ“‹ å¸®åŠ©å‘½ä»¤
â“ /web_help - æ˜¾ç¤ºæœ¬å¸®åŠ©ä¿¡æ¯
   åˆ«åï¼š/ç½‘é¡µåˆ†æå¸®åŠ©, /ç½‘é¡µåˆ†æå‘½ä»¤
   ç¤ºä¾‹ï¼š/web_help

ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
- æ‰€æœ‰å‘½ä»¤æ”¯æŒTabè¡¥å…¨ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
- å‘½ä»¤å‚æ•°æ”¯æŒæç¤ºåŠŸèƒ½
- å¯ä»¥è‡ªå®šä¹‰å‘½ä»¤åˆ«å

ğŸ”§ é…ç½®æç¤ºï¼š
- åœ¨AstrBotç®¡ç†é¢æ¿ä¸­å¯ä»¥é…ç½®æ’ä»¶çš„å„é¡¹åŠŸèƒ½
- æ”¯æŒè‡ªå®šä¹‰å‘½ä»¤åˆ«å
- å¯ä»¥è°ƒæ•´åˆ†æç»“æœæ¨¡æ¿å’Œæ˜¾ç¤ºæ–¹å¼
"""

        yield event.plain_result(help_text)
        logger.info("æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯")

    @filter.command("web_config", alias={"ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶çš„è¯¦ç»†é…ç½®ä¿¡æ¯"""
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {"âœ… å·²å¯ç”¨" if self.llm_enabled else "âŒ å·²ç¦ç”¨"}
- åˆ†ææ¨¡å¼: {self.analysis_mode}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {"âœ… å·²å¯ç”¨" if self.auto_analyze else "âŒ å·²ç¦ç”¨"}

**å¹¶å‘å¤„ç†è®¾ç½®**
- æœ€å¤§å¹¶å‘æ•°: {self.max_concurrency}
- åŠ¨æ€å¹¶å‘æ§åˆ¶: {"âœ… å·²å¯ç”¨" if self.dynamic_concurrency else "âŒ å·²ç¦ç”¨"}
- ä¼˜å…ˆçº§è°ƒåº¦: {"âœ… å·²å¯ç”¨" if self.enable_priority_scheduling else "âŒ å·²ç¦ç”¨"}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {"âœ… å·²å¯ç”¨" if self.enable_emoji else "âŒ å·²ç¦ç”¨"}
- æ˜¾ç¤ºç»Ÿè®¡: {"âœ… å·²å¯ç”¨" if self.enable_statistics else "âŒ å·²ç¦ç”¨"}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å‘é€å†…å®¹ç±»å‹: {self.send_content_type}
- å¯ç”¨æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.enable_screenshot else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {"âœ… å·²å¯ç”¨" if self.enable_cache else "âŒ å·²ç¦ç”¨"}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""

        yield event.plain_result(config_info)

    @filter.command("test_merge", alias={"æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½"""
        from astrbot.api.message_components import Node, Nodes, Plain

        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = self._get_group_id(event)

        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []

            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")],
            )
            nodes.append(title_node)

            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node1)

            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node2)

            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")

    @filter.command("group_blacklist", alias={"ç¾¤é»‘åå•", "é»‘åå•"})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return

            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"

            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"

            yield event.plain_result(blacklist_info)
            return

        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""

        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")

        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")

        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return

            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result(
                "æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear"
            )

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_cache", alias={"ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += (
                f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            )

            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"

            yield event.plain_result(cache_info)
            return

        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""

        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(
                f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª"
            )

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_mode", alias={"åˆ†ææ¨¡å¼", "ç½‘é¡µåˆ†ææ¨¡å¼"})
    async def manage_analysis_mode(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†ææ¨¡å¼"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰æ¨¡å¼
        if len(message_parts) <= 1:
            mode_names = {
                "auto": "è‡ªåŠ¨åˆ†æ",
                "manual": "æ‰‹åŠ¨åˆ†æ",
                "hybrid": "æ··åˆæ¨¡å¼",
            }
            mode_info = "**å½“å‰åˆ†ææ¨¡å¼**\n\n"
            mode_info += f"- æ¨¡å¼: {mode_names.get(self.analysis_mode, self.analysis_mode)} ({self.analysis_mode})\n"
            mode_info += (
                f"- è‡ªåŠ¨åˆ†æ: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}\n\n"
            )
            mode_info += "ä½¿ç”¨ `/web_mode <æ¨¡å¼>` åˆ‡æ¢æ¨¡å¼\n"
            mode_info += "æ”¯æŒçš„æ¨¡å¼: auto, manual, hybrid, LLMTOOL"

            yield event.plain_result(mode_info)
            return

        # è§£ææ¨¡å¼å‚æ•°
        mode = message_parts[1].lower() if len(message_parts) > 1 else ""
        valid_modes = ["auto", "manual", "hybrid", "LLMTOOL"]

        # éªŒè¯æ¨¡å¼æ˜¯å¦æœ‰æ•ˆ
        if mode not in valid_modes:
            yield event.plain_result(f"æ— æ•ˆçš„æ¨¡å¼ï¼Œè¯·ä½¿ç”¨: {', '.join(valid_modes)}")
            return

        # æ›´æ–°åˆ†ææ¨¡å¼
        self.analysis_mode = mode
        self.auto_analyze = mode == "auto"

        yield event.plain_result(f"âœ… å·²åˆ‡æ¢åˆ° {mode} æ¨¡å¼")

    @filter.command("web_export", alias={"å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result(
                "è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json"
            )
            return

        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"

        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(
                f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}"
            )
            return

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []

        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache_manager.memory_cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return

            for url, cache_data in self.cache_manager.memory_cache.items():
                export_results.append({"url": url, "result": cache_data["result"]})
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all

            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self.message_handler.check_cache(url)
            if cached_result:
                export_results.append({"url": url, "result": cached_result})
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")

                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with self.analyzer as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return

                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(
                            event, content_data["content"]
                        )
                        translated_content_data = content_data.copy()
                        translated_content_data["content"] = translated_content
                        analysis_result = await self.llm_analyzer.analyze_with_llm(
                            event, translated_content_data
                        )
                    else:
                        analysis_result = await self.llm_analyzer.analyze_with_llm(
                            event, content_data
                        )

                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.enable_specific_extraction:
                        specific_content = analyzer.extract_specific_content(
                            html, url, self.extract_types
                        )
                        if specific_content:
                            # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                            analysis_result = self._add_specific_content_to_result(
                                analysis_result, specific_content
                            )

                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append(
                        {
                            "url": url,
                            "result": {
                                "url": url,
                                "result": analysis_result,
                                "screenshot": None,
                            },
                        }
                    )

        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            import json
            import os
            import time

            from astrbot.api.message_components import File, Plain

            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url_obj = export_results[0]["url"]
                from urllib.parse import urlparse

                parsed = urlparse(url_obj)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"

            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"

            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")

            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)

            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(timestamp)
                    ),
                    "total_results": len(export_results),
                    "results": [],
                }

                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]

                    json_data["results"].append(
                        {
                            "url": url,
                            "analysis_result": result_data["result"],
                            "has_screenshot": result_data["screenshot"] is not None,
                        }
                    )

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                File(file=file_path, name=os.path.basename(file_path)),
            ]

            yield event.chain_result(message_chain)

            logger.info(
                f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·"
            )

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")

    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶"""
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = "\n".join(self.group_blacklist)
            # è·å–å½“å‰group_settingsé…ç½®
            group_settings = self.config.get("group_settings", {})
            # æ›´æ–°group_blacklist
            group_settings["group_blacklist"] = group_text
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config["group_settings"] = group_settings
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")

    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content: è¦ç¿»è¯‘çš„å†…å®¹

        Returns:
            ç¿»è¯‘åçš„å†…å®¹
        """
        if not self.enable_translation:
            return content

        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, "llm_generate"):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(
                    umo=umo
                )

            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content, target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = (
                    f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œ"
                    f"ä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"
                )

            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id, prompt=prompt
            )

            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content

    def _add_specific_content_to_result(
        self, analysis_result: str, specific_content: dict
    ) -> str:
        """å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­

        Args:
            analysis_result: å½“å‰çš„åˆ†æç»“æœ
            specific_content: ç‰¹å®šå†…å®¹å­—å…¸

        Returns:
            æ›´æ–°åçš„åˆ†æç»“æœ
        """
        try:
            # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
            specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

            # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "images" in specific_content and specific_content["images"]:
                specific_content_str += (
                    f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                )
                for img in specific_content["images"]:
                    img_url = img.get("url", "")
                    alt_text = img.get("alt", "")
                    if alt_text:
                        specific_content_str += f"- {img_url} (alt: {alt_text})\n"
                    else:
                        specific_content_str += f"- {img_url}\n"

            # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "links" in specific_content and specific_content["links"]:
                specific_content_str += (
                    f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                )
                for link in specific_content["links"][:5]:
                    specific_content_str += (
                        f"- [{link['text']}]({link['url']})\n"
                    )

            # æ·»åŠ è§†é¢‘é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "videos" in specific_content and specific_content["videos"]:
                specific_content_str += (
                    f"\nğŸ¬ è§†é¢‘é“¾æ¥ ({len(specific_content['videos'])}):\n"
                )
                for video in specific_content["videos"]:
                    video_url = video.get("url", "")
                    video_type = video.get("type", "video")
                    specific_content_str += f"- {video_url} (type: {video_type})\n"

            # æ·»åŠ éŸ³é¢‘é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if "audios" in specific_content and specific_content["audios"]:
                specific_content_str += (
                    f"\nğŸµ éŸ³é¢‘é“¾æ¥ ({len(specific_content['audios'])}):\n"
                )
                for audio in specific_content["audios"]:
                    specific_content_str += f"- {audio}\n"

            # æ·»åŠ å¼•ç”¨å—ï¼ˆå¦‚æœæœ‰ï¼‰
            if "quotes" in specific_content and specific_content["quotes"]:
                specific_content_str += (
                    f"\nğŸ’¬ å¼•ç”¨å— ({len(specific_content['quotes'])}):\n"
                )
                for quote in specific_content["quotes"][:3]:
                    quote_text = quote.get("text", "")
                    author = quote.get("author", "")
                    if author:
                        specific_content_str += f"> {quote_text} â€” {author}\n\n"
                    else:
                        specific_content_str += f"> {quote_text}\n\n"

            # æ·»åŠ æ ‡é¢˜åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if "headings" in specific_content and specific_content["headings"]:
                specific_content_str += (
                    f"\nğŸ“‘ æ ‡é¢˜åˆ—è¡¨ ({len(specific_content['headings'])}):\n"
                )
                for heading in specific_content["headings"]:
                    level = heading.get("level", 1)
                    text = heading.get("text", "")
                    heading_id = heading.get("id", "")
                    indent = "  " * (level - 1)
                    if heading_id:
                        specific_content_str += (
                            f"{indent}#{level} {text} (id: {heading_id})\n"
                        )
                    else:
                        specific_content_str += f"{indent}#{level} {text}\n"

            # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼‰
            if "code_blocks" in specific_content and specific_content["code_blocks"]:
                specific_content_str += (
                    f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                )
                for code_block in specific_content["code_blocks"][:2]:
                    code = code_block.get("code", "")
                    language = code_block.get("language", "")
                    specific_content_str += f"``` {language}\n{code}\n```\n"

            # æ·»åŠ è¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰
            if "tables" in specific_content and specific_content["tables"]:
                specific_content_str += (
                    f"\nğŸ“Š è¡¨æ ¼ ({len(specific_content['tables'])}):\n"
                )
                for table in specific_content["tables"][:2]:
                    headers = table.get("headers", [])
                    rows = table.get("rows", [])
                    specific_content_str += f"\nè¡¨æ ¼:\n"
                    if headers:
                        specific_content_str += (
                            f"| {' | '.join(headers)} |\n"
                            f"| {' | '.join(['---' for _ in headers])} |\n"
                        )
                    for row in rows:
                        specific_content_str += f"| {' | '.join(row)} |\n"

            # æ·»åŠ åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if "lists" in specific_content and specific_content["lists"]:
                specific_content_str += (
                    f"\nğŸ“‹ åˆ—è¡¨ ({len(specific_content['lists'])}):\n"
                )
                for list_item in specific_content["lists"][:2]:
                    list_type = list_item.get("type", "ul")
                    items = list_item.get("items", [])
                    specific_content_str += f"\nåˆ—è¡¨ ({list_type}):\n"
                    for item in items:
                        if list_type == "ol":
                            specific_content_str += f"1. {item}\n"
                        else:
                            specific_content_str += f"- {item}\n"

            # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "meta" in specific_content and specific_content["meta"]:
                meta_info = specific_content["meta"]
                specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                for key, value in meta_info.items():
                    if value:
                        specific_content_str += f"- {key}: {value}\n"

            # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
            return analysis_result + specific_content_str
        except Exception as e:
            logger.warning(f"æ·»åŠ ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return analysis_result

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")
